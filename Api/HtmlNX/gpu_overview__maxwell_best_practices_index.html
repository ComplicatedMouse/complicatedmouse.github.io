<!-- HTML header for doxygen 1.9.1-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Maxwell Best Practices | NintendoSDK API Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="openclose.js"></script>
<script type="text/javascript" src="searchapi.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="stylesheet.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">NintendoSDK API Reference
   &#160;<span id="projectnumber">14.1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="_page_graphics_for_n_x.html">Graphics Environment for NX</a></li>  </ul>
</div>
</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Maxwell Best Practices </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_1">1. Overview</a></li>
<li class="level1"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2">2. Graphics</a><ul><li class="level2"><a href="#gpuOverview_MaxwellBestPractices_earlyZ">2.1. EarlyZ</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_EarlyZRequirements">2.1.1. EarlyZ Requirements</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_earlyZGlobalConditions">2.1.1.1. Global Conditions</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_1_1_2">2.1.1.2. Local Conditions</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_earlyZOverride">2.1.1.3. EarlyZ Override</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_1_2">2.1.2. Best Practices</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_1_2_1">2.1.2.1. Force Enable EarlyZ Whenever Possible</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_zcull">2.2. ZCull</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_ZCullTest">2.2.1. Per-Tile Depth Testing and Culling</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_2_2">2.2.2. ZCull Buffer Updates</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_2_3">2.2.3. Performance Considerations</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_2_3_1">2.2.3.1. ZCull ZF32 Compression</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_2_4">2.2.4. Optimization Options</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_2_4_1">2.2.4.1. Disable Compression for ZF32 Depth Buffers in Reversed-Z Configuration</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_fastZ">2.3. FastZStencil</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_FBcompression">2.4. Framebuffer Compression</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_4_1">2.4.1. Best Practices</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_4_1_1">2.4.1.1. Enable Framebuffer Compression</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_4_2">2.4.2. Optimization Options</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_RMWblend">2.4.2.1. Enable Tiled Caching for Blending</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_fastClear">2.5. Fast Clear (Zero Bandwidth Clear)</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_fastGS">2.6. Fast Geometry Shader</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_GfxShader">2.7. Shader</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_1">2.7.1. Best Practices</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_ShaderGroupSSBOwrites">2.7.1.1. Group SSBO Writes</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_1_2">2.7.1.2. Reduce Graphics-Compute Context Switches</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_1_3">2.7.1.3. Avoid Dynamic Indexing into Arrays</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_1_4">2.7.1.4. Avoid Using imageStore() on Framebuffer Compressed Surfaces</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_1_5">2.7.1.5. Compile Together All Shaders for Each Program</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_SSBOtoUBO">2.7.1.6. Replace SSBOs with UBOs</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_2">2.7.2. Optimization Options</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_ShaderReduceRegSpill">2.7.2.1. Reduce Register Spills</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_BatchTexInsts">2.7.2.2. Batch Texture Instructions</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_TextureGathers">2.7.2.3. Use Texture Gathers</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_Shuffle">2.7.2.4. Use Shuffle or Subgroup Intrinsics</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_Fullscreen">2.7.2.5. Use Fullscreen Rendering Pass to Optimize Thread Occupancy</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_6">2.7.2.6. Use Half Float Types</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_OverrideSubtile">2.7.2.7. Override Fragment Warp Subtile Size</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_HalfFloat">2.7.3. Half Float (FP16) Operations</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_HalfFloatBenefits">2.7.3.1. Benefits of FP16 Operations</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_3_2">2.7.3.2. HW and Instructions with FP16 Support</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_3_3">2.7.3.3. FP16 Limitations</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_3_4">2.7.3.4. Guideline on FP16 Usage in Shaders</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_3_5">2.7.3.5. Expected Peformance Gain</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_StaticAnalysis">2.7.4. Static Analysis</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_4_1">2.7.4.1. Precautions</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_WarpLimiters">2.7.4.2. Warp Throughput Limiters</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_8">2.8. Vertex Processing</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_8_1">2.8.1. Best Practices</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_PackVertexAttribs">2.8.1.1. Pack Attributes</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_OptimizeIndex">2.8.1.2. Optimize Index Buffer</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_8_1_3">2.8.1.3. Optimize Vertex Attribute Array Layout</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_8_2">2.8.2. Optimization Options</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_CBF">2.8.2.1. Enable Cull-Before-Fetch (CBF)</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_VertexWarpCulling">2.8.2.2. Enable Vertex Warp Culling</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_9">2.9. Primitive Processing</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_9_1">2.9.1. Optimization Options</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_PrimClipCull">2.9.1.1. Reduce Clipped and Culled Primitive Overhead</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_ISBE">2.10. ISBE</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_10_1">2.10.1. Best Practices</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_ISBEworldSpace">2.10.1.1. Optimize World-Space Shaders</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_ISBEscreenSpace">2.10.1.2. Optimize Screen-Space Processing</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_10_2">2.10.2. Optimization Options</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_ISBEreduceCull">2.10.2.1. Reduce Culled Primitive Overhead</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_ISBEreduceUsage">2.10.2.2. Reduce Count of World-Space Shader Inputs and Outputs</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_TRAM">2.11. TRAM</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_11_1">2.11.1. Optimization Options</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_TRAMreduceVaryings">2.11.1.1. Reduce Count of Varyings</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_12">2.12. Data Copies</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_12_1">2.12.1. Best Practices</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_12_1_1">2.12.1.1. Batch Copy Operations</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_12_1_2">2.12.1.2. Use ENGINE_2D Copy Flag for Buffer to Texture Copies</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_13">2.13. Texturing</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_13_1">2.13.1. Best Practices</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_TextureFBcompression">2.13.1.1. Enable Framebuffer Compression</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_TextureFormats">2.13.1.2. Use Optimal Formats</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_TextureFiltering">2.13.1.3. Use Optimal Filtering Mode</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_Texture3Dto2D">2.13.1.4. Use 2D Array instead of 3D Textures</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_13_2">2.13.2. Optimization Options</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_InitializeCompressed">2.13.2.1. Initialize with Framebuffer-Compressed Values</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_TexTiledCaching">2.13.2.2. Experiment with Tiled Caching</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_13_2_3">2.13.2.3. Others</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_TextureCopies">2.13.3. Texture Copies</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_13_3_1">2.13.3.1. Texture Copy with 3D Pipeline</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_13_3_2">2.13.3.2. Texture Copy with RSTR2D Unit</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_13_3_3">2.13.3.3. Additional Performance Considerations</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_14">2.14. Debug Group Annotation</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_14_1">2.14.1. Optimization Options</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_14_1_1">2.14.1.1. Remove Debug Group Annotations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="level1"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3">3. Compute</a><ul><li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_ComputeGraphicsInteractions">3.1. Interaction with Graphics Workload</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3_2">3.2. Shaders</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3_2_1">3.2.1. UBO</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3_2_2">3.2.2. SSBO</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3_2_3">3.2.3. Shared Memory</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p><a class="anchor" id="md_gpuOverview_MaxwellBestPractices"></a></p>
<h1><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_1"></a>
1. Overview</h1>
<p>This document contains suggestions on how to optimize graphics and compute workloads for the Maxwell GPU architecture as used in the NX SOC. It contains highlights of hardware features and suggestions on how best to write applications to maximize performance. For a detailed description of the GPU architecture please refer to <a class="el" href="gpu_overview__maxwell_technical_overview_index.html">Maxwell Technical Overview</a>.</p>
<p>The recommendations for some sections below are separated into two groups, <em>Best Practices</em> and <em>Optimization Options</em>:</p><ul>
<li><em>Best Practices</em> suggestions should be followed whenever possible as they should never lead to performance loss.</li>
<li><em>Optimization Options</em> are optional suggestions which may lead to improved performance for certain cases but may also to lead to performance loss in others. Experimentation and verification are necessary.</li>
</ul>
<p>Please refer to <a class="el" href="gpu_overview__maxwell_profiling_guide_index.html">Maxwell PerfWorks Profiling Guide</a> for suggestions on how to identify and verify cases which might benefit from specific optimization options.</p>
<h1><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2"></a>
2. Graphics</h1>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_earlyZ"></a>
2.1. EarlyZ</h2>
<p>In NX GPU, the read-test-write of the Z-buffer is always done only once in one of two places within the graphics pipeline:</p>
<ul>
<li>If EarlyZ is enabled, the read-test-write of the Z-buffer occurs before the fragment shader execution by the SM.</li>
<li>If EarlyZ is disabled (LateZ mode), the read-test-write of the Z-buffer occurs after the fragment shader execution by the SM.</li>
</ul>
<p>The EarlyZ mode can have a positive impact on performance by culling fragments earlier in the pipeline, saving the SM from processing fragments that would not be visible anyway.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_EarlyZRequirements"></a>
2.1.1. EarlyZ Requirements</h3>
<p>Because EarlyZ mode determines the final value of each fragment's Z value before the fragment shader stage it can only be enabled if no subsequent stage in the pipeline could affect the Z or coverage value of a fragment.</p>
<p>Refer to the tables below for conditions that enable and disable EarlyZ mode.</p>
<p>The conditions are divided into 2 groups:</p><ul>
<li>Global conditions which always disable EarlyZ if not satisfied.</li>
<li>Local conditions which can disable EarlyZ only when depth and/or stencil surfaces can be modified by the draw call.</li>
</ul>
<p>The method to check if EarlyZ is enabled for a draw call is the following:</p><ul>
<li>If any global condition is not satisfied then EarlyZ is disabled.</li>
<li>If all global conditions are satisfied and the draw call can not modify depth and/or stencil surfaces, then the local conditions can be ignored and EarlyZ is enabled.</li>
<li>If all global conditions are satisfied and the draw call can modify depth and/or stencil surfaces, then EarlyZ is enabled only if all local conditions are satisfied.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_earlyZGlobalConditions"></a>
2.1.1.1. Global Conditions</h4>
<p>All conditions in the table below must be satisfied to enable EarlyZ.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Operation   </th><th class="markdownTableHeadNone">EarlyZ    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Fragment shader does not modify depth   </td><td class="markdownTableBodyNone">enabled    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Fragment shader does not modify SSBO   </td><td class="markdownTableBodyNone">enabled    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Fragment shader does not call imageStore() or imageAtomic*()   </td><td class="markdownTableBodyNone">enabled   </td></tr>
</table>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_1_1_2"></a>
2.1.1.2. Local Conditions</h4>
<p>If the draw call can not modify depth and/or stencil values, the local conditions in the table below can be ignored. Otherwise all conditions below must be satisfied to enable EarlyZ.</p>
<p>A draw call cannot modify depth value if at least one of the following is true:</p><ul>
<li>No depth render target is bound.</li>
<li>Depth test is disabled (When depth test is disabled, writes to depth are also disabled).</li>
<li>Writes to depth buffer is disabled.</li>
<li>Depth function is set to EQUAL.</li>
</ul>
<p>A draw call cannot modify stencil value if at least one of the following is true:</p><ul>
<li>No stencil render target is bound.</li>
<li>Stencil test is disabled.</li>
<li>Stencil mask bits are all 0s for front and back stencil tests.</li>
<li>Any combination of stencil func and stencil ops for front and back stencil tests which do not result in modifying any stencil values.</li>
</ul>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Operation   </th><th class="markdownTableHeadNone">EarlyZ    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Alpha test disabled   </td><td class="markdownTableBodyNone">enabled    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Alpha to coverage disabled   </td><td class="markdownTableBodyNone">enabled    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Fragment shader does not modify gl_SampleMask   </td><td class="markdownTableBodyNone">enabled    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Fragment shader does not contain discards   </td><td class="markdownTableBodyNone">enabled   </td></tr>
</table>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_earlyZOverride"></a>
2.1.1.3. EarlyZ Override</h4>
<p>Using the <em>early_fragment_tests</em> layout qualifier in the fragment shader can also be used to force EarlyZ mode. If a fragment shader declares the <em>early_fragment_tests</em> qualifer, both global and local conditions above will be ignored and EarlyZ mode will always be enabled. For details please refer to the OpenGL extension ARB_shader_image_load_store.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_1_2"></a>
2.1.2. Best Practices</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_1_2_1"></a>
2.1.2.1. Force Enable EarlyZ Whenever Possible</h4>
<p>Additional performance can be gained by forcibly enabling EarlyZ mode when it is known that the rendering result will not be affected.</p>
<p>For example, fragment shaders that modify SSBOs or execute image load/store operations will implicitly disable EarlyZ. However in cases where these modifications are only necessary for fragments that pass the depth test, enabling <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_earlyZOverride">EarlyZ override</a> may increase performance.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_zcull"></a>
2.2. ZCull</h2>
<p>The ZCull unit culls 16x16 pixel tiles during rasterization in between the coarse raster and fine raster phase.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_ZCullTest"></a>
2.2.1. Per-Tile Depth Testing and Culling</h3>
<p>ZCull maintains an occluder map (ZCull buffer) in memory to store the minimum or maximum Z values for each tile location depending on the current ZCull depth direction set by the driver.</p>
<p>Based on this depth direction, ZCull will trivially accept or conservatively reject incoming pixels from raster, potentially saving further stages in the graphics pipeline from doing additional work.</p>
<p>In NVN the ZCull depth direction command is generated from the depthValue parameter of nvnCommandBufferClearDepthStencil() when the depthMask parameter is true using the following formula: </p><div class="image">
<img src="MaxwellBestPractices_img01.jpg" alt=""/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 1: ZCull Depth Direction</b> </div><p> <br  />
</p>
<p>Everytime the command to set ZCull direction is executed, the entire ZCull buffer is invalidated.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_2_2"></a>
2.2.2. ZCull Buffer Updates</h3>
<p>The ZCull buffer is updated when the ZROP unit reads from the target depth buffer or writes a fully-covered tile.</p>
<p>The ZCull buffer update data is generated by the ZROP unit when executing any one of the following operations:</p><ul>
<li>Read operations from depth buffer for depth testing or read-modify-write expansion (<a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_perf_read_modify_write">8.9.7. Read-modify-write expands</a> in the Maxwell Technical Overview documentation).</li>
<li>Write operations to the depth buffer which cover an entire ROP tile.</li>
</ul>
<p>In other words, The ZCull buffer will not be updated at any of following conditions:</p><ul>
<li>ZROP is skipped (Both of depth test and write are disabled or depth buffer is not bound)</li>
<li>Depth test is disabled and ZROP writes partially covered uncompressed ROP tile (no depth buffer read, no entire tile write, and no read-modify-write expand).</li>
</ul>
<p>The ZCull buffer update operation occurs at different locations in the graphics pipeline depending on whether EarlyZ is active or not:</p><ul>
<li>If EarlyZ is active, the ZROP operation occurs before the fragment is processed by the fragment shader.</li>
<li>If EarlyZ is not active (LateZ), the ZROP operation occurs after the fragment is processed by the fragment shader.</li>
</ul>
<p>The update data will be sent to the ZCull unit via the PROP unit. There is no guarantee of minimum and maximum delay when the update data will arrive and be processed by ZCull to be used in future culling operations.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_2_3"></a>
2.2.3. Performance Considerations</h3>
<p>ZCull's performance can be reduced or disabled entirely by the following factors:</p>
<ul>
<li>Changing ZCull depth direction or switching render targets causes the entire ZCull buffer to be invalidated, effectively losing all accumulated depth information.</li>
<li>ZCull will be less effective if the ZCull buffer contains information that is not up to date. This is much more likely to happen when EarlyZ is not enabled (LateZ mode).</li>
<li>ZCull can be ineffective (unable to cull) if the ZCull depth direction does not match the currently set depth compare function.</li>
</ul>
<p>The following table summarizes different conditions and their impact on ZCull performance.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone"></th><th class="markdownTableHeadNone">Buffer Invalidated   </th><th class="markdownTableHeadNone">Reduced Performance   </th><th class="markdownTableHeadNone">Unable to cull    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">ZCull-GFX Depth Direction Mismatch   </td><td class="markdownTableBodyNone">No   </td><td class="markdownTableBodyNone">No   </td><td class="markdownTableBodyNone">Yes    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">EarlyZ Not Enabled   </td><td class="markdownTableBodyNone">No   </td><td class="markdownTableBodyNone">Yes   </td><td class="markdownTableBodyNone">No    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">ZCull Depth Direction Changed   </td><td class="markdownTableBodyNone">Yes   </td><td class="markdownTableBodyNone">No   </td><td class="markdownTableBodyNone">No    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Render Target Changed   </td><td class="markdownTableBodyNone">Yes   </td><td class="markdownTableBodyNone">No   </td><td class="markdownTableBodyNone">No   </td></tr>
</table>
<p>For details on how the ZCull depth direction is set refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_ZCullTest">2.2.1. Per-Tile Depth Testing and Culling</a>.</p>
<p>For conditions on when EarlyZ is enabled please refer <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_EarlyZRequirements">2.1.1. EarlyZ Requirements</a>.</p>
<p>When switching render targets it is possible to save the current ZCull buffer for future restore using the NVN ZCull save and restore functions. Please refer to the NVN programming guide section "ZCull
Save and Restore" for a detailed description.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_2_3_1"></a>
2.2.3.1. ZCull ZF32 Compression</h4>
<p>In addition to the factors described above, ZCull efficiency for 32-bit floating point depth buffer formats, ZF32, can be further affected by the combination of the following:</p>
<ul>
<li>The ZCull depth compare direction.</li>
<li>Whether or not ZF32 compression is enabled.</li>
</ul>
<p>Please refer to the following table for ZCull efficiency at various ranges of depth values:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone"></th><th class="markdownTableHeadNone">ZF32 Compression Enabled   </th><th class="markdownTableHeadNone">ZF32 Compression Disabled    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">LESS   </td><td class="markdownTableBodyNone">Good near 1.0, bad &lt;0.25 or &gt;1.0   </td><td class="markdownTableBodyNone">Good near 1.0, bad &gt;1.0 (worse than compression enabled)    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">GREATER   </td><td class="markdownTableBodyNone">Bad &lt;0.25   </td><td class="markdownTableBodyNone">Good near 0.0   </td></tr>
</table>
<p>In NVN, ZF32 compression can be controlled by nvnCommandBufferSetZCullZF32CompressionEnable(), and it is enabled by default. The API only affects ZF32 depth buffers, and never affects integer depth buffers. Since toggling compression is a heavy operation and requires a subsequent manual clear of the depth buffer, it should be done as sparingly as possible.</p>
<p>If ZF32 format is used, it is recommended to profile rendering to detect if the ZCull depth direction in combination with compression and the ranges of depth values rendered are causing reduced performance.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_2_4"></a>
2.2.4. Optimization Options</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_2_4_1"></a>
2.2.4.1. Disable Compression for ZF32 Depth Buffers in Reversed-Z Configuration</h4>
<p>When using ZF32 depth buffer in reversed-Z configuration, enabling compression will cause reduced ZCull efficiency at depth ranges less than 0.25. If a large portion of the geometry rendered falls into this depth range, disabling ZF32 compression should help increase ZCull efficiency.</p>
<p>If the ZCull depth direction is always GREATER for all of the bound ZF32 depth buffers, we recommend disabling ZCull ZF32 compression once at application start up.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_fastZ"></a>
2.3. FastZStencil</h2>
<p>Maxwell GPU has a fast 4x performance mode when rendering only writes to the Z and stencil buffer.</p>
<p>Refer to the table below for enabling and disabling conditions of FastZStencil mode. Please note that any single "disabled" condition is enough to turn off FastZStencil mode. On the other hand, although an "enabled" condition will not disable FastZStencil, it is still possible that something else will.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Condition   </th><th class="markdownTableHeadNone">FastZStencil    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">EarlyZ disabled   </td><td class="markdownTableBodyNone">disabled    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">EarlyZ enabled   </td><td class="markdownTableBodyNone">enabled    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Shader writes color   </td><td class="markdownTableBodyNone">disabled    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Shader does not write color   </td><td class="markdownTableBodyNone">enabled   </td></tr>
</table>
<p>Disabling shader color writes can be done in several ways:</p>
<ul>
<li>Binding NULL fragment shader</li>
<li>Binding NULL for color targets</li>
<li>Disable color channels</li>
</ul>
<p>For conditions necessary for enabling EarlyZ please refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_EarlyZRequirements">2.1.1. EarlyZ Requirements</a>.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_FBcompression"></a>
2.4. Framebuffer Compression</h2>
<p>Maxwell GPU in NX SOC supports framebuffer compression, which can significantly reduce memory traffic. This feature is automatically enabled for all supported formats so applications need only be aware of enabling compression when creating textures as follows:</p><ul>
<li>Set the COMPRESSIBLE bit in <a class="el" href="structnvn_1_1_memory_pool_flags.html" title="Specifies properties of memory pools created from a memory pool builder.">nvn::MemoryPoolFlags</a> when creating the memory pool where the texture will be stored.</li>
<li>Set the COMPRESSIBLE bit in <a class="el" href="structnvn_1_1_texture_flags.html" title="Identifies special properties of texture allocations.">nvn::TextureFlags</a> when creating the texture.</li>
</ul>
<p>For further details please refer to section 3, "Memory Pool Objects", and section 5.1, "Texture Initialization", of the NVN programming guide.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_4_1"></a>
2.4.1. Best Practices</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_4_1_1"></a>
2.4.1.1. Enable Framebuffer Compression</h4>
<p>In almost all cases there is no downside to enabling framebuffer-compression so if possible it is always recommended to create framebuffer textures which are framebuffer-compressible. Please see below for one possible exception.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_4_2"></a>
2.4.2. Optimization Options</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_RMWblend"></a>
2.4.2.1. Enable Tiled Caching for Blending</h4>
<p>Certain patterns of blending operations on framebuffer-compressed surfaces may result in reduced operations:</p>
<p>Accesses to compressed color surfaces are done in units of 256 bytes called ROP tiles. When the GPU's CROP unit needs to blend the fragment shader output with the destination color and the area to be blended only partially covers the destination ROP tile, CROP may need to do a read of the destination tile followed by a decompress before blending. It will then attempt to recompress the tile before writing out the result back to memory. In draw calls resulting in many partial tile updates, the latency increase due to the extra memory operations may reduce performance to less than if uncompressed surfaces are used. For further details on ROP tile expansions please refer to the section <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_perf_read_modify_write">8.9.7. Read-modify-write expands</a> in the Maxwell Technical Overview documentation.</p>
<p>In these cases it is suggested to try enabling tiled caching which may improve performance by increasing L2 cache hitrate and reducing memory operation latencies.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_fastClear"></a>
2.5. Fast Clear (Zero Bandwidth Clear)</h2>
<p>Maxwell GPU in NX SOC supports a special framebuffer-compression format which can be cleared with almost zero memory bandwidth requirement if the clear color or clear depth value has been registered to the NVN driver. Some common clear color and depth values are already pre-registered.</p>
<p>For further details on how to register clear color and depth values, and the list of pre-registered values, please refer to the NVN programming guide document on section "10.16. Framebuffer Clears".</p>
<p>For further details on framebuffer-compression please see <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_FBcompression">2.4. Framebuffer Compression</a>.</p>
<p>The table below summarizes the conditions that could enable/disable Fast Clear operations. Please note that any single "disabled" condition is enough to turn off Fast Clear. On the other hand, although an "enabled" condition will not disable Fast Clear, it is possible that something else will.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Condition   </th><th class="markdownTableHeadNone">Fast Clear    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Surface format is compressible   </td><td class="markdownTableBodyNone">Enabled    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Surface format is not compressible   </td><td class="markdownTableBodyNone">Disabled    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Clear color is registered   </td><td class="markdownTableBodyNone">Enabled    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Clear color is not registerd   </td><td class="markdownTableBodyNone">Disabled   </td></tr>
</table>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_fastGS"></a>
2.6. Fast Geometry Shader</h2>
<p>Maxwell GPU contains the following geometry shader features which can be used together to improve performance for certain types of rendering tasks:</p>
<ul>
<li>High performance geometry shader when certain conditions are met.</li>
<li>Render a primitive to multiple layers, each with its own unique viewport (multicast).</li>
<li>Swizzling of vertex position members for each target surface.</li>
</ul>
<p>These features can be used to accelerate certain types of rendering tasks such as cubemap rendering, cascaded shadow mapping, and multi-resolution rendering.</p>
<p>The table below summarizes the conditions that could enable/disable fast geometry shader. Please note that any single "disabled" condition is enough to turn off the feature. On the other hand, although an "enabled" condition will not disable it, it is possible that something else will.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Condition   </th><th class="markdownTableHeadNone">Fast GS    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Geometry shader writes vertex attributes   </td><td class="markdownTableBodyNone">disabled    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Geometry shader does not write vertex attributes   </td><td class="markdownTableBodyNone">enabled    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Geometry shader writes primitive attributes   </td><td class="markdownTableBodyNone">enabled    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Geometry shader emits new vertices   </td><td class="markdownTableBodyNone">disabled    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Geometry shader does not emit new vertices   </td><td class="markdownTableBodyNone">enabled   </td></tr>
</table>
<p>Please refer to the following OpenGL extensions also for details on how to enable FastGS and viewport multicast:</p>
<ul>
<li>NV_geometry_shader_passthrough for details and examples on FastGS enabled geometry shaders.</li>
<li>NV_viewport_array2 for details and examples on viewport multicast. Example for vertex position swizzling is included in the GameWork CubemapRendering sample.</li>
</ul>
<p>Fast geometry shader can also be used to efficiently implement primitive culling as described in <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_PrimClipCull">2.9.1.1. Reduce Clipped and Culled Primitive Overhead</a>.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_GfxShader"></a>
2.7. Shader</h2>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_1"></a>
2.7.1. Best Practices</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_ShaderGroupSSBOwrites"></a>
2.7.1.1. Group SSBO Writes</h4>
<p>Writes to SSBO should be done in a sequential manner if possible. This should help improve caching performance.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_1_2"></a>
2.7.1.2. Reduce Graphics-Compute Context Switches</h4>
<p>Avoid frequent switching between graphics and compute shaders. This causes the GPU to execute expensive context switches. Recommend batching graphics shader draw calls and compute shader dispatch calls as much as possible.</p>
<p>For further details on the mechanism of graphics and compute context switches please see <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance_ChannelSwitch">8.4.2. Channels and Subchannels</a>.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_1_3"></a>
2.7.1.3. Avoid Dynamic Indexing into Arrays</h4>
<p>Avoid accessing arrays with dynamic indexes as this can reduce performance. For example, the following is not recommended: </p><div class="fragment"><div class="line">int index = ... ;</div>
<div class="line">...</div>
<div class="line">float foo = array[index];</div>
</div><!-- fragment --><p> Instead when possible use a constant index: </p><div class="fragment"><div class="line">float foo = array[2];</div>
</div><!-- fragment --><p> Variable indexed arrays can cause the following to occur:</p><ul>
<li>The compiler may store the array in local memory instead of registers which would cause loads and stores to be much slower.</li>
<li>If the value of the array is used as a conditional variable this may cause performance reducing thread divergence in the warp execution when actually not necessary.</li>
</ul>
<p>In addition variable indexed arrays cannot be used for shader specialization.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_1_4"></a>
2.7.1.4. Avoid Using imageStore() on Framebuffer Compressed Surfaces</h4>
<p>GLSL imageStore() function writes data through the texture pipeline which is not a framebuffer-compression aware unit. Please see <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_highlight_FRAME_BUFFER_COMPRESSION">here</a>.</p>
<p>It is recommended to avoid using imageStore() on framebuffer-compressed surfaces for the following reasons:</p>
<ul>
<li>The output values from the texture pipeline will be in an uncompressed format which may contribute to increased XBAR congestion.</li>
<li>The affected destination tiles will be left in an uncompressed state which will reduce future access performance.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_1_5"></a>
2.7.1.5. Compile Together All Shaders for Each Program</h4>
<p>It is recommended to compile all shaders within a program together. GLSLC shader compiler has an optimization feature where it will attempt to detect and eliminate unnecessary code from a program by examining dependencies between all of its shaders. However this can only work if all the shaders within a program is compiled together.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_SSBOtoUBO"></a>
2.7.1.6. Replace SSBOs with UBOs</h4>
<p>Whenever possible it is recommended to use UBOs over SSBOs for the following reasons:</p>
<ul>
<li>In most common cases UBOs have a faster datapath backed with more levels of caching.</li>
<li>Non-indexed UBOs can be accessed and used directly from most instructions without requiring an extra load instruction and an extra temporary register storage.</li>
</ul>
<p>Using UBOs instead of SSBOs may help improve access latency and reduce register usage.</p>
<p>The exception cases where SSBOs may be preferable over UBOs are when the data access pattern has a very high divergence between the threads of a warp. For example, if almost all 32 threads of a warp is guaranteed to access different values, then SSBO may be preferable.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_2"></a>
2.7.2. Optimization Options</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_ShaderReduceRegSpill"></a>
2.7.2.1. Reduce Register Spills</h4>
<p>If the shader has register spills to scratch memory, these reads and writes to and from scratch memory to registers add additional processing cycles and workload for the texture pipeline. GLSLC's spillControl compile option and/or pragmas can be used to eliminate spill although this can affect SM performance in other ways. Please refer to the GLSLC programming manual for further details.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_BatchTexInsts"></a>
2.7.2.2. Batch Texture Instructions</h4>
<p>The GLSLC compile option <em>prioritizeConsecutiveTextureInstructions</em> can be enabled for compute and fragment shaders to batch texture fetches as much as possible. In some conditions this may help improve performance by increasing texture cache hit rate. For further details please refer to the GLSLC programming guide.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_TextureGathers"></a>
2.7.2.3. Use Texture Gathers</h4>
<p>When possible use textureGatherOffsets() to explicitly parallelize texture sampling.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_Shuffle"></a>
2.7.2.4. Use Shuffle or Subgroup Intrinsics</h4>
<p>Reads and writes to memory buffers shared between threads of a warp may be replaced by shuffle or subgroup intrinsics to replace memory read/write operations with more efficient register copies.</p>
<p>When applicable, the usage of shuffle or subgroup intrinsics can also help reduce branch divergence between threads in a warp by unifying the branch condition to guarantee that all threads in the warp branches to the same location.</p>
<p>Please refer to the NV_shader_thread_shuffle and KHR_shader_subgroup extensions for further details.</p>
<p>Please refer to <a class="el" href="gpu_overview__maxwell_profiling_guide_index.html#gpuOverview_MaxwellProfiling_guide_ThreadOccupancy">5.8.3. Thread Occupancy</a> for further explanation about how branch divergence can negatively impact performance.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_Fullscreen"></a>
2.7.2.5. Use Fullscreen Rendering Pass to Optimize Thread Occupancy</h4>
<p>Draw calls rendering primitives which cover small and/or irregular areas of pixels can have reduced performance due to low thread occupancy (see <a class="el" href="gpu_overview__maxwell_profiling_guide_index.html#gpuOverview_MaxwellProfiling_guide_ThreadOccupancy">5.8.3. Thread Occupancy</a>). Draw calls that render a single screen filling primitive on the other hand, are guaranteed to launch fully occupied fragment shader warps. Consider shifting heavy shader workloads into fullscreen passes if possible to maximize thread occupancy.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_6"></a>
2.7.2.6. Use Half Float Types</h4>
<p>Please refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_HalfFloat">2.7.3. Half Float (FP16) Operations</a> for further details.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_OverrideSubtile"></a>
2.7.2.7. Override Fragment Warp Subtile Size</h4>
<p>Fragment shader warps are launched and retired together in groups called subtiles. Please refer to <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance_SM_Neighbors">8.2.1. SM and Its Neighbors</a> for more details.</p>
<p>By default the driver decides the count of fragment warps for each subtile, but the application can override this value by calling nvnCommandBufferOverrideSubtileSize(). Please refer to the NVN programming guide for further details.</p>
<p>The subtile size of fragment warps can potentially affect performance as described in sub-sections below.</p>
<p><a class="anchor" id="autotoc_md56"></a> </p><h5>Fragment Warp Drain</h5>
<p>Because all fragment warps within a subtile must be retired together, subtiles containing fragment warps with highly differing workloads may lead to some of the warps having to wait for others to finish. This in turn delays the freeing of output registers which may be needed to launch new warps. In these cases, it is recommended to experiment with reducing the subtile size to reduce this delay.</p>
<p>Please refer to <a class="el" href="gpu_overview__maxwell_profiling_guide_index.html#gpuOverview_MaxwellProfiling_guide_FragmentWarpPixelDrain">5.8.2.4. Fragment Warp Pixel Drain</a> for further details and how to check if this issue is potentially affecting performance.</p>
<p><a class="anchor" id="autotoc_md57"></a> </p><h5>Texture L1 Cache Locality</h5>
<p>All fragment warps within a subtile shares the same TEX L1 data cache. In cases where the L1 texture cache hit rate is low, experimenting with increased subtile size may improve the hit rate by forcing more warps to share the same L1 cache.</p>
<p>Please refer to the profiling guide section <a class="el" href="gpu_overview__maxwell_profiling_guide_index.html#gpuOverview_MaxwellProfiling_guide_TEX">5.7. TEX (Texture)</a> for how to check the TEX L1 cache hit rate.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_HalfFloat"></a>
2.7.3. Half Float (FP16) Operations</h3>
<p>This section contains additional information on FP16 mode as described in <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance_FP16">8.2.3.4. FP16 support</a>. The following subsections will provide further details on:</p><ul>
<li>What are the potential benefits of FP16 mode.</li>
<li>Which HW and instructions support FP16 operations and what are the potential benefits for each.</li>
<li>What are the various HW and compiler limitations of FP16 operations in NX.</li>
<li>How to enable FP16 operations in shaders and guidelines on how to maximize their benefits.</li>
<li>Expected performance improvement that can be gained by converting shader to use FP16 operations based on past experience along with some shader examples.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_HalfFloatBenefits"></a>
2.7.3.1. Benefits of FP16 Operations</h4>
<p>Using FP16 operations in NX may lead to the following benefits:</p>
<ul>
<li>Vectorization of FP16 operations when applicable. As detailed in <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance_FP16">8.2.3.4. FP16 support</a>, the NX SM unit is capable of combining 2 FP16 operations into a single instruction. Under ideal conditions this may lead to FP16 operations having double the peak performance of FP32 instructions as shown in the "SM FMAs/clk (fp16)" entry of table 3 in the section <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance">8.1. PERFORMANCE SUMMARY</a>. However only certain FP16 instructions support vectorization and those instructions may not be vectorized under certain conditions. Please refer to sections below for details.</li>
<li>Reduced register usage. Because two FP16 values can be packed into a single FP32 register, FP16 operations require only half the number of registers of its FP32 counterpart. This reduction may lead to increased performance by increasing warp occupancy (the number of warps that can be simultaneously active within the same SM sub-partition), which in turn can lead to better latency hiding of high latency operations such as texture fetches. Reduced register usage may also help compiler to better optimize shaders in various ways. Please refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_StaticAnalysis">2.7.4. Static Analysis</a> for more details on how register limitation and warp occupancy can affect performance.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_3_2"></a>
2.7.3.2. HW and Instructions with FP16 Support</h4>
<p>NX supports FP16 operations with the possibility of vectorizing some operations under certain conditions. Discrete GPUs earlier than the Pascal generation however, do not support FP16 operations. On these discrete GPUs shader FP16 operations are emulated using FP32 instructions and will not gain the benefits listed above.</p>
<p>Please refer to the table below for the list of NX instructions which supports FP16 operation along with their possible benefits.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">SM Instruction   </th><th class="markdownTableHeadNone">Supports Vectorization (SIMD mode)   </th><th class="markdownTableHeadNone">Reduced Register Usage    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">TEXS   </td><td class="markdownTableBodyNone">No   </td><td class="markdownTableBodyNone">Yes    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">HADD2   </td><td class="markdownTableBodyNone">Yes (1)   </td><td class="markdownTableBodyNone">Yes    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">HMUL2   </td><td class="markdownTableBodyNone">Yes (1)   </td><td class="markdownTableBodyNone">Yes    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">HFMA2   </td><td class="markdownTableBodyNone">Yes (1)   </td><td class="markdownTableBodyNone">Yes    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">HSET2   </td><td class="markdownTableBodyNone">Yes (1)   </td><td class="markdownTableBodyNone">Yes    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">HSETP2   </td><td class="markdownTableBodyNone">Yes (1)   </td><td class="markdownTableBodyNone">Yes   </td></tr>
</table>
<p>(1) Cannot be vectorized under certain conditions</p>
<p>Although executing FP16 operations which do not support vectorization will not increase performance, it will still gain the reduced register usage benefits.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_3_3"></a>
2.7.3.3. FP16 Limitations</h4>
<p>Although some subset of FP16 instructions supports vectorization as listed above, there are some HW limitations which would prevent these instructions to operate in vectorized (SIMD) mode as listed below:</p>
<ul>
<li>If some or all of the input and output of the instruction is of FP32 precision then it would be necessary for the SM to do inline conversion from FP16 to FP32. This inline conversion prevents vectorization.</li>
<li>In NX uniform and SSBO accesses are treated as loads of FP32 values regardless if the uniform/SSBO type is FP16 or FP32. This means that FP16 instructions which accesses uniform/SSBO values must do inline conversions to FP16 which prevents vectorization.</li>
<li>Similar to uniforms and SSBO, all input/output attribute accesses are also treated as FP32 accesses in NX, preventing vectorization of FP16 instructions which reads and/or writes from/to attributes.</li>
<li>For 2 FP16 instructions to be vectorized it must have identical input and output modifiers.</li>
<li>For 2 FP16 instructions to be vectorized it must have identical predicate condition.</li>
</ul>
<p>Additionally glslc currently behaves in the following manner:</p>
<ul>
<li>If an input or output of an FP16 instruction is of precision FP32, glslc will favor inline conversion from FP32 to FP16 instead of generating an extra conversion instruction. This would prevent vectorization of the FP16 instruction. There are a few cases where vectorization is favored over inline conversion at the expense of extra instruction(s) but these cases are the minority.</li>
<li>Any FP16 instructions with predicates are not vectorized.</li>
<li>FP16 instruction pairs with different input/output modifiers will not be vectorized. The compiler will make no attempt to add extra instructions to make vectorization possible.</li>
<li>FP16 instruction pairs which belong to different blocks in the control flow will not be vectorized.</li>
<li>GLSLC will compile certain intrinsic functions into instructions that do not support FP16. Using the outputs of these instructions in FP16 operations requires inline conversion which will prevent vectorization.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_3_4"></a>
2.7.3.4. Guideline on FP16 Usage in Shaders</h4>
<p>To enable FP16 operations declare the default float precision to mediump or lowp. Both will map to FP16.</p>
<p>Enabling FP16 operations has the potential to yield performance gain and typically does not lead to performance losses.</p>
<p>However, in some situations, the increased warp count resulting from enabling FP16 can cause cache thrashing, leading to poorer performance than the FP32 baseline. A good rule of thumb is as follows:</p><ul>
<li>Experiment with enabling FP16 in compositing shaders. Rationale: such shaders are memory bound and require peak warps to saturate memory. They do not rely on cache locality for performance.</li>
<li>Experiment with enabling FP16 in math heavy shaders (warp throughput limiters identified as "issue" or "fp" by shader statistic analysis).</li>
</ul>
<p>Because of the potential for performance loss we recommend testing for regression whenever experimenting with enabling FP16 operations for a shader.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_3_5"></a>
2.7.3.5. Expected Peformance Gain</h4>
<p>As mentioned above under ideal conditions FP16 instruction can achieve twice the throughput of its FP32 counterpart. The following shader shows 2x performance in FP16 mode:</p>
<div class="fragment"><div class="line">precision mediump float;</div>
<div class="line"> </div>
<div class="line">void main() {</div>
<div class="line"> </div>
<div class="line">  // conversion from fp32 to fp16</div>
<div class="line">  vec4 vColor = vtxColor;</div>
<div class="line">  vec4 temp1 = value1;</div>
<div class="line">  vec4 temp2 = value2.xyxy;</div>
<div class="line"> </div>
<div class="line">  // math bound shader</div>
<div class="line">  for (int i = 0; i &lt; 1024; i++) {</div>
<div class="line"> </div>
<div class="line">      // pure fp16 mul instructions</div>
<div class="line">      // can be vectorized</div>
<div class="line">      vColor *= temp1;</div>
<div class="line">      vColor += temp2;</div>
<div class="line">      vColor *= temp1;</div>
<div class="line">      vColor += temp2;</div>
<div class="line">      vColor *= temp1;</div>
<div class="line">      vColor += temp2;</div>
<div class="line">      vColor *= temp1;</div>
<div class="line">      vColor += temp2;</div>
<div class="line">      vColor *= temp1;</div>
<div class="line">      vColor += temp2;</div>
<div class="line">      vColor *= temp1;</div>
<div class="line">      vColor += temp2;</div>
<div class="line">      vColor *= temp1;</div>
<div class="line">      vColor += temp2;</div>
<div class="line">      vColor *= temp1;</div>
<div class="line">      vColor += temp2;</div>
<div class="line">  }</div>
<div class="line"> </div>
<div class="line">  // conversion back to fp32</div>
<div class="line">  outColor = vColor*vtxColor;</div>
<div class="line">}</div>
</div><!-- fragment --><p>However due to HW and compiler limitations, it is possible to see very small or no performance improvement for typical game shaders which are either not bottlenecked by register usage or are not optimized to take advantage of the conditions for enabling vectorization as described above.</p>
<p>For example, the following shader does not gain any benefit from conversion to FP16 mode due to inline conversion necessary because normalize() generates instructions that do not support FP16:</p>
<div class="fragment"><div class="line">precision mediump float;</div>
<div class="line">in mediump vec3 vNormal;</div>
<div class="line"> </div>
<div class="line">void main(void) {</div>
<div class="line">  mediump vec3 color = normalize(vNormal);</div>
<div class="line">  gl_FragColor.rgb = color;</div>
<div class="line">  gl_FragColor.a = 1.0;</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">Disassembly:</div>
<div class="line"> </div>
<div class="line">     IPA.PASS   R5, a[0x7c]                   &amp;wr=0                     ?OFF_DECK;</div>
<div class="line">     MUFU.RCP   R5, R5                        &amp;req={0} &amp;wr=0            ?OFF_DECK_YIELD6;</div>
<div class="line">     IPA        R0, a[0x80], R5               &amp;req={0} &amp;wr=0            ?WAIT1;</div>
<div class="line">     IPA        R1, a[0x84], R5               &amp;wr=1                     ?WAIT1;</div>
<div class="line">     IPA        R2, a[0x88], R5               &amp;wr=2                     ?OFF_DECK;        // no fp16 variant of IPA</div>
<div class="line">     HMUL2.MRG_H0 R3, R0.F32, R0.F32          &amp;req={0}                  ?WAIT6;</div>
<div class="line">     HFMA2.MRG_H1 R3, R1.F32, R1.F32, R3.H0_H0  &amp;req={1}                ?WAIT6;</div>
<div class="line">     HFMA2.F32  R4, R2.F32, R2.F32, R3.H1_H1  &amp;req={2}                  ?WAIT2;</div>
<div class="line">     MUFU.RSQ   R3, R4                        &amp;wr=0                     ?OFF_DECK_YIELD6; // no fp16 variant of RSQ</div>
<div class="line">     HMUL2.F32  R0, R3.F32, R0.F32            &amp;req={0}                  ?WAIT1_END_GROUP;</div>
<div class="line">     HMUL2.F32  R1, R3.reuse.F32, R1.F32                                ?WAIT1;</div>
<div class="line">     HMUL2.F32  R2, R3.F32, R2.F32                                      ?WAIT1;</div>
<div class="line">     MOV32I     R3, 1.0                                                 ?PAIR;</div>
<div class="line">     EXIT                                                               ?OFF_DECK;</div>
</div><!-- fragment --><h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_StaticAnalysis"></a>
2.7.4. Static Analysis</h3>
<p>The shader compiler GLSLC can output performance statistics during shader compilation. These can be used as hints to find bottlenecks and what area to concentrate on to improve shader performance. This section will discuss in detail the meaning of various performance statistics items along with suggestions for optimizations. For details on how to output performance statistics please refer to the GLSLC documentation.</p>
<p>Please also refer to <a class="el" href="gpu_overview__maxwell_profiling_guide_index.html">Maxwell PerfWorks Profiling Guide</a> for run-time dynamic profiling of performance statistics using PerfWorks.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_4_1"></a>
2.7.4.1. Precautions</h4>
<p>Because of the nature of compile time shader analysis there are a few points to keep in mind when using analysis results for optimization:</p>
<ul>
<li>Static shader analysis can give performance data which affect shader performance only. Note that overall rendering performance may be bottlenecked by something other than shaders such as vertex attribute fetch, primitive processing, etc. In these cases shader static analysis data will not be helpful for optimizing rendering performance.</li>
<li>Because of the static nature of the analysis used to generate these data, they may not be accurate for shaders whose behavior can change dynamically depending on runtime conditions. For example, static analysis of shaders containing many conditional branches and loops may not yield accurate data.</li>
<li>Finally, it is also important to note that optimizing for one particular item/operation may worsen the performance of another operation and lead to no improvement or even worse overall performance. It is important to iterate and consider the balance of all limiters.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_WarpLimiters"></a>
2.7.4.2. Warp Throughput Limiters</h4>
<p>Warp throughput limiters are estimates of the number of warps that can be executed per-clock-cycle for particular types of shader operation. These estimates take into account only the SM resources available and the shader program's workload for that type of operation, ignoring any interactions between different types of operations competing for the same SM resources. When possible, for performance optimization it is recommended to supplement these static estimates with dynamic profiling statistics (see <a class="el" href="gpu_overview__maxwell_profiling_guide_index.html">Maxwell PerfWorks Profiling Guide</a>).</p>
<p>Taking into account the above limitations, the operation type with the lowest warp throughput estimate can be considered the likeliest to be the bottleneck. Each operations type's throughput limiter are further described below along with some suggestions on how to improve them.</p>
<p><a class="anchor" id="autotoc_md58"></a> </p><h5>issue, fp, half, ipa, transcendental</h5>
<p>The issue warp limiter indicates warp throughput as limited by shader math instructions of all type.</p>
<p>Warp throughput as limited by specific types of math instructions are indicated by the following:</p><ul>
<li>fp - 32-bit floating point math operations.</li>
<li>half - 16-bit floating point math operations.</li>
<li>ipa - varying interpolation operations.</li>
<li>transcendental - transcendental math operations (SIN, COS, LOG, etc).</li>
</ul>
<p>Suggestions:</p>
<p>When possible use mediump float precision. This would assist the compiler in generating SIMD fp16 instructions which can be executed simultaneously in pairs to reduce math operation latency. Additionally, SIMD fp16 instructions also has the benefit of reduced power consumption.</p>
<p>When there are branches and conditionals, group math instructions as close as possible with the code that is using the result of their calculations. This will assist the compiler in grouping instructions in such a way to avoid executing operations in branches that are not taken. For example:</p>
<p>In the "good" case below, calculation for value_a or value_b is done only when their corresponding branches are taken:</p>
<div class="fragment"><div class="line">if (do_a) {</div>
<div class="line">  value_a = calculate_a();</div>
<div class="line">  value = value_a + value_c;</div>
<div class="line">} else {</div>
<div class="line">  value_b = calculate_b();</div>
<div class="line">  value = value_b + value_c;</div>
<div class="line">}</div>
<div class="line">return value;</div>
</div><!-- fragment --><p>In the "bad" case below, calculation for both value_a and value_b can potentially be done even though only one of them will be used:</p>
<div class="fragment"><div class="line">value_a = calculate_a();</div>
<div class="line">value_b = calculate_b();</div>
<div class="line">if (do_a) {</div>
<div class="line">  value = value_a + value_c;</div>
<div class="line">} else {</div>
<div class="line">  value = value_b + value_c;</div>
<div class="line">}</div>
<div class="line">return value;</div>
</div><!-- fragment --><p><a class="anchor" id="autotoc_md59"></a> </p><h5>shared</h5>
<p>This indicates warp throughput as limited by the following operations:</p><ul>
<li>Vertex attributes and varying loads and stores.</li>
<li>Shared memory loads and stores for compute shaders.</li>
</ul>
<p>Suggestions:</p><ul>
<li>When possible combine attributes and varyings. For example 2 vec2 attributes can be combined into a single vec4.</li>
<li>Use uniform to pass value instead of vertex attributes if possible.</li>
<li>For compute shaders, replace shared memory loads and stores with the shuffleNV() instruction which allows for exchange of values between threads in the same warp without accessing shared memory. Please refer to NV_shader_thread_shuffle extension for details.</li>
</ul>
<p><a class="anchor" id="autotoc_md60"></a> </p><h5>controlFlow</h5>
<p>This indicates warp throughput as limited by the number of branches.</p>
<p>Suggestions:</p><ul>
<li>Eliminate branches whenever possible.</li>
</ul>
<p><a class="anchor" id="autotoc_md61"></a> </p><h5>texLoadStore</h5>
<p>This indicates warp throughput as limited by the following operations.</p>
<p>Global memory loads and stores:</p><ul>
<li>Texture fetches.</li>
<li>SSBO loads and stores.</li>
<li>Image loads and stores.</li>
</ul>
<p>Local memory loads and stores:</p><ul>
<li>Load and stores of register values spilled to local memory.</li>
</ul>
<p>Suggestions:</p><ul>
<li>Reduce texture operations.</li>
<li>Use textureGather() and textureGatherOffsets() for better parallelization.</li>
<li>Use uniforms instead of SSBO.</li>
<li>Minimize the number of local variables.</li>
</ul>
<p><a class="anchor" id="autotoc_md62"></a> </p><h5>reg</h5>
<p>This indicates warp throughput as limited by the register usage count per-warp and the shader register usage pattern.</p>
<p>In every sub-partition there are 512 registers that must be shared by all active warps.</p>
<p>If the number of registers needed by each warp is too high it could limit throughput by reducing warp occupancy, which is the ratio of the number of possible warps that can be active and the maximum number of active warps possible in the HW.</p>
<p>If the shader register usage pattern requires that registers are needed for long periods then throughput can be limited by the increased shader latency because there is not enough free register to launch additional warps.</p>
<p>The reg warp limiter captures how throughput is limited by both warp occupancy and shader latency.</p>
<p>Suggestions:</p>
<p>When possible use mediump float precision. Two fp16 float values can be packed into a single fp32 register to reduce register usage. Additionally, SIMD fp16 instructions also has the benefit of reduced power consumption.</p>
<p>Improve shader latency by assisting compiler in batching together high latency operations such as texture fetches. One of the ways the compiler can batch high latency operations is by loop unrolling. To help compiler in loop unrolling we suggest the following ways to write loops:</p><ul>
<li>Loops with known iteration count and increment operator of add or subtract can be unrolled completely. For example: <div class="fragment"><div class="line">for (uint i = 0; i &lt; 5; i++) {</div>
<div class="line">  ..</div>
<div class="line">}</div>
</div><!-- fragment --></li>
<li>Loops with increment operator other than add or substract cannot be unrolled. If possible avoid creating such loops. For example the following loop cannot be unrolled: <div class="fragment"><div class="line">for (uint i = 1; i &lt; 10; i *= 2) {</div>
<div class="line">  ..</div>
<div class="line">}</div>
</div><!-- fragment --></li>
<li>Compiler is less efficient at unrolling for loops which contain if conditional blocks. If possible avoid them. For example the following loop cannot be efficiently unrolled: <div class="fragment"><div class="line">for (..) {</div>
<div class="line">  ..</div>
<div class="line">  if (..) {</div>
<div class="line">    ..</div>
<div class="line">  }</div>
<div class="line">  ..</div>
<div class="line">}</div>
</div><!-- fragment --></li>
<li>In general avoid complex conditions if possible to assist in unrolling.</li>
</ul>
<p>How successful the compiler has been in unrolling loops can be discovered by checking the values of "Loop data", a part of the performance statistics output:</p><ul>
<li>PartiallyUnrolled indicates the number of loops which the compiler was able to unroll to some degree but not completely.</li>
<li>Nonunrolled indicates the number of loops which the compiler was not able to unroll.</li>
</ul>
<p><a class="anchor" id="autotoc_md63"></a> </p><h5>warp</h5>
<p>This indicates warp throughput as limited by the time it takes to launch the warp within the SM sub-partition.</p>
<p>Suggestions:</p>
<p>Usually this limiter becomes the bottleneck when the workload of the warp is so light that its execution is dominated by the warp launch itself. As such optimizing the shader itself will usually not lead to performance improvements. Instead, we suggest searching for bottlenecks other than shaders (i.e. fixed function processing, attribute fetches, etc).</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_8"></a>
2.8. Vertex Processing</h2>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_8_1"></a>
2.8.1. Best Practices</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_PackVertexAttribs"></a>
2.8.1.1. Pack Attributes</h4>
<p>NX can fetch up to 2 vec4 attributes per clock cycle regardless of the size of that attributes. This means that attributes declared as scalar, vec2, and vec3 would still require 1 clock to fetch each, leading to reduced efficiency of 75%, 50%, and 25% respectively. Vertex fetch efficiency can be increased by packing attributes into vec4s as much as possible.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_OptimizeIndex"></a>
2.8.1.2. Optimize Index Buffer</h4>
<p>The NX primitive distributor (PD) builds batches containing up-to 32 triangles or 32 unique indices. Reordering your index-buffer so that triangles sharing vertices are close enough to fit in a batch allows to generate less batches and therefore improves performance.</p>
<p>The PD is optimized for 1 new index per triangle. Triangles ordered in strip/fan pattern or forming an hilbert-ish curve will allow the PD to function at maximum speed.</p>
<p>There are several known algorithms that allow to optimize the index-buffer. We recommend using the forsyth algorithm [For06] for the following reasons:</p><ul>
<li>it generates few number of batches</li>
<li>it generates hilbert-ish triangle order maximizing PD performance</li>
<li>it is fast to run</li>
</ul>
<p>[For06] <a href="http://tomforsyth1000.github.io/papers/fast_vert_cache_opt.html">http://tomforsyth1000.github.io/papers/fast_vert_cache_opt.html</a></p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_8_1_3"></a>
2.8.1.3. Optimize Vertex Attribute Array Layout</h4>
<p>This optimization is generally done after optimizing the index-buffer by reordering vertices in the same order as they come in the index-buffer so that vertex-data is accessed roughly in linear order. Groups of vertex attributes that are accessed together in shader should be interleaved in memory.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_8_2"></a>
2.8.2. Optimization Options</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_CBF"></a>
2.8.2.1. Enable Cull-Before-Fetch (CBF)</h4>
<p>Cull-Before-Fetch is an optimization which splits vertex shaders into VSa and VSb where:</p><ul>
<li>VSa fetches only attributes necessary to calculate position which are then frustum culled.</li>
<li>VSb executes the rest of the vertex shader only for the primitives that survive.</li>
</ul>
<p>In draw calls where a significant portion of primitives are frustum culled, enabling CBF can have the following benefits because frustum-culled primitives are discarded at the vertex shader stage:</p><ul>
<li>Reduce other world-space shader invocation count.</li>
<li>Reduce fixed-function primitive processing workload.</li>
<li>Reduce ISBE usage.</li>
</ul>
<p>However CBF also has its own disadvantages, as it has the additional overhead of running the vertex shader twice.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_VertexWarpCulling"></a>
2.8.2.2. Enable Vertex Warp Culling</h4>
<p>Vertex warp culling is an optimization enabled by the shader compiler by inserting a frustum culling code within the vertex shader. If, and only if, all vertices within the vertex shader warp lies outside the frustum, then the entire warp will be discarded. This has the following potential benefits because frustum-culled primitives are discarded at the vertex shader stage:</p><ul>
<li>Reduce other world-space shader invocation count.</li>
<li>Reduce fixed-function primitive processing workload.</li>
<li>Reduce ISBE usage.</li>
</ul>
<p>Please refer to the GLSLC documentation for further details on vertex warp culling.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_9"></a>
2.9. Primitive Processing</h2>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_9_1"></a>
2.9.1. Optimization Options</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_PrimClipCull"></a>
2.9.1.1. Reduce Clipped and Culled Primitive Overhead</h4>
<p>Draw calls in which large percentages of primitives are clipped or culled could waste processing cycles and resources for:</p><ul>
<li>Fetching of clipped/culled primitives' vertex attributes.</li>
<li>Invocations of world-space shaders (vertex, tessellation, and geometry) for clipped/culled primitives.</li>
<li>Allocations of ISBE entries to hold attributes and shader input/output for clipped/culled primitives.</li>
</ul>
<p>The following methods can be used to reduce this overhead:</p><ul>
<li>If possible, tune the application's LOD system to filter out as much as possible the clipped or culled primitives sent to the GPU for drawing.</li>
<li>Add a primitive clipping and/or culling pre-pass early in the frame to remove the culled primitives from the list to be drawn on later passes.</li>
</ul>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_ISBE"></a>
2.10. ISBE</h2>
<p>ISBEs are intermediate buffers used to pass data within world-space processing and between world-space and screen-space pipelines. Please see <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance_ISBE">8.5.4. Inter-Stage Buffer Entries (ISBEs)</a>.</p>
<p>ISBEs are used to store:</p><ul>
<li>Vertex attributes.</li>
<li>Outputs and inputs of world-space shaders (vertex, tessellation, geometry).</li>
</ul>
<p>ISBEs are allocated per-scalar value so packing of its contents into vectors do not affect allocation efficiency.</p>
<p>NX contains 48 KBytes of ISBE buffer per-TPC which must be shared by all world-space processing occurring within that TPC. When there is no available entry, allocations would stall and potentially impact performance.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_10_1"></a>
2.10.1. Best Practices</h3>
<p>The recommendations for world-space and screen-space optimizations below are, of course, desirable in general. They are listed here to point out their relationship to ISBE allocations.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_ISBEworldSpace"></a>
2.10.1.1. Optimize World-Space Shaders</h4>
<p>Reducing world-space shader invocations and optimizing execution time for each invocation will reduce the demand and usage duration of ISBE entries. Reducing the number of attributes per-shader-stage will also reduce the size of ISBEs required.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_ISBEscreenSpace"></a>
2.10.1.2. Optimize Screen-Space Processing</h4>
<p>ISBE entries allocated as output of the last world-space shader stage cannot be deallocated until their corresponding primitives have been clipped, culled, and the entries' values are written to the beta circular buffer in L2. The lack of space within L2 due to heavy screen-space workload can delay this deallocation.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_10_2"></a>
2.10.2. Optimization Options</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_ISBEreduceCull"></a>
2.10.2.1. Reduce Culled Primitive Overhead</h4>
<p>ISBE entries are not freed until the primitives associated with them have been clipped and culled. Culled primitives are also a waste of ISBE entries allocated to them. Reducing this culled primitive overhead alleviates ISBE allocation stalls. Please see <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_PrimClipCull">2.9.1.1. Reduce Clipped and Culled Primitive Overhead</a>.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_ISBEreduceUsage"></a>
2.10.2.2. Reduce Count of World-Space Shader Inputs and Outputs</h4>
<p>By reducing the count of world-space shader (vertex, tessellation, geometry shader) inputs and outputs, less ISBE entries will be required overall. Note that each ISBE entry is treated as an fp32 value so there is no benefit in using lower precision data types.</p>
<p>When possible the compiler will attempt to re-use an input ISBE entry of a shader stage for its output. For this reason, when attempting to reduce the ISBE usage of a shader, it is best to target the interface which requires the largest number of entries. For example, if a shader has 3 vec4 input and 4 vec4 outputs, the total entries required is likely to be (4 vec4 * 4 scalar/vec4) or 16 entries. Reducing the input will not be effective as the same 16 entries would still be required to store the output.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_TRAM"></a>
2.11. TRAM</h2>
<p>TRAM entries are allocated to store the primitive attribute plane equations necessary to interpolate varying values for the fragment shader. Similar to ISBEs, a TRAM entry is allocated per-scalar value so the packing of varying into vectors does not affect overall usage.</p>
<p>NX contains 16 KBytes of TRAM buffer per-TPC which must be shared by all screen-space processing occurring within that TPC. When there is no available entry, allocations would stall and potentially impact performance in the following ways:</p><ul>
<li>In screen-space, TRAM allocation stalls could reduce SM's performance by delaying its ability to start fragment shader warps.</li>
<li>TRAM allocation stall could increase the time it takes to drain the beta circular buffer, which in turn could stall the world-space pipeline.</li>
</ul>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_11_1"></a>
2.11.1. Optimization Options</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_TRAMreduceVaryings"></a>
2.11.1.1. Reduce Count of Varyings</h4>
<p>Note that each TRAM entry is allocated as an fp32 value so there is no benefit in using lower precision data types.</p>
<p>TRAM allocation stalls can be improved by reducing the fragment shader varying count:</p><ul>
<li>If there are varyings whose values are constant for all fragments, pass them as uniforms or SSBO instead.</li>
<li>If possible, instead of calculating a varying in a world-space shader, interpolate their values in screen-space instead.</li>
</ul>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_12"></a>
2.12. Data Copies</h2>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_12_1"></a>
2.12.1. Best Practices</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_12_1_1"></a>
2.12.1.1. Batch Copy Operations</h4>
<p>NVN provides four commands allowing applications to schedule GPU operations copying data from one buffer object or texture to another buffer object or texture. For further details please refer to the "Data Copy Commands" section of the NVN programming guide.</p>
<p>When these copy commands are submitted to a queue (via a command set), they are executed in order relative to commands before and after the copy. The copy commands will see the results of any commands sent to the queue prior to the copy, and any commands sent to the queue after the copy will see the results of the copy. To guarantee this the 3D pipeline will be idled before executing the copy command and subsequent command after the copy will not be started until the copy operation has been completed. Because idling the 3D pipeline may reduce performance, we recommend batching data copies to share this overhead over as many copy operations as possible.</p>
<p>Please also refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_TextureCopies">2.13.3. Texture Copies</a> for more details and suggestions about texture copies.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_12_1_2"></a>
2.12.1.2. Use ENGINE_2D Copy Flag for Buffer to Texture Copies</h4>
<p>The nvnCommandBufferCopyBufferToTexture() has the option of using the 2D engine for executing the copy operation. Because the 2D engine has better performance than the copy engine, it is recommended whenever possible to copy using the 2D engine by enabling the NVN_COPY_FLAGS_ENGINE_2D_BIT in the CopyFlags parameter.</p>
<p>Enabling the 2D engine copy path also has the benefit compressing compressible texture format which would increase texture sampling performance.</p>
<p>For more information about the color formats that are supported for 2D engine copying, please see the NVN programming guide.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_13"></a>
2.13. Texturing</h2>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_13_1"></a>
2.13.1. Best Practices</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_TextureFBcompression"></a>
2.13.1.1. Enable Framebuffer Compression</h4>
<p>Whenever possible allocate textures which support framebuffer-compression for render targets. Please refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_FBcompression">2.4. Framebuffer Compression</a>.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_TextureFormats"></a>
2.13.1.2. Use Optimal Formats</h4>
<p>Texture pipeline operations have different latencies depending on the texture format. In general texture formats with more bits per texel will run at reduced performance. Whenever possible use the most efficient format. Please refer to the section <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_perf_TEXTURE">8.8. TEXTURE</a> for performance characteristics of various formats. For HDR render targets it is recommended to experiment with R11G11B10F format instead of the commonly used RGBA16F for improved performance.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_TextureFiltering"></a>
2.13.1.3. Use Optimal Filtering Mode</h4>
<p>Texture filtering operation in the GPU is optimized for bilinear interpolation. Additional interpolation will reduce throughput through serialization. For example trilinear filtering is bilinear filtering done twice. Please refer to the section <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_perf_TEXTURE">8.8. TEXTURE</a> for performance characteristics of various filtering modes.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_Texture3Dto2D"></a>
2.13.1.4. Use 2D Array instead of 3D Textures</h4>
<p>If filtering is not needed over the R texture coordinate direction, using 2D array will lead to the more efficient bilinear filtering (see above section).</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_13_2"></a>
2.13.2. Optimization Options</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_InitializeCompressed"></a>
2.13.2.1. Initialize with Framebuffer-Compressed Values</h4>
<p>For framebuffer-compressible textures whose contents are initialized directly by the application, it is recommended to initialize them with framebuffer-compressed values using nvnCopyBufferToTexture() with the ENGINE_2D bit set in the copy flag parameter.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_TexTiledCaching"></a>
2.13.2.2. Experiment with Tiled Caching</h4>
<p>Enabling tiled caching with small tile sizes may improve the texture unit's L1 cache hitrate and reduce texture fetch latency. Please refer to the NVN programming guide for details</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_13_2_3"></a>
2.13.2.3. Others</h4>
<p>The following shader optimization suggestions are also related to texture pipeline performance. They are listed here for reference:</p><ul>
<li><a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_ShaderReduceRegSpill">2.7.2.1. Reduce Register Spills</a></li>
<li><a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_BatchTexInsts">2.7.2.2. Batch Texture Instructions</a></li>
<li><a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_TextureGathers">2.7.2.3. Use Texture Gathers</a></li>
</ul>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_TextureCopies"></a>
2.13.3. Texture Copies</h3>
<p>This section contains guidelines for ensuring correctness and optimizing performance of texture copies. There are 2 methods available in NVN to copy the content of one texture to another which will be discussed below.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_13_3_1"></a>
2.13.3.1. Texture Copy with 3D Pipeline</h4>
<p>Texture content can be copied by executing a draw call which uses a fragment shader that samples the content of the source texture and writes it out to the destination texture bound as a render target. To ensure correctness a barrier needs to be inserted before the copy texture draw call if there are previous draw calls which write to the source texture. Similarly a barrier needs to be inserted after the copy texture draw call if there are subsequent draw calls which samples from the destination texture.</p>
<p>The barrier bits which are used as arguments to nvnCommandBufferBarrier() in the cases above must be carefully selected to ensure both correctness and optimal performance. Incorrect barrier bits will cause incorrect result while sub-optimal barrier bits may reduce performance by causing the 3D pipeline to stall longer than necessary.</p>
<p>The following are general recommendations:</p><ul>
<li>Barriers with INVALIDATE_TEXTURE bits are usually necessary before and after the operation to ensure that the copy will not fetch old cached values from the source texture and subsequent draw calls do not fetch old cached values from the destination texture.</li>
<li>Barriers with one of ORDER_PRIMITIVES, ORDER_FRAGMENTS, or ORDER_FRAGMENTS_TILED bits are also necessary to ensure that the copy doesn't start until previous rendering commands that write to the source have finished and that subsequent rendering commands don't start until the copy is completed.</li>
<li>ORDER_FRAGMENTS should be sufficient if the textures can be written to and sampled from fragment shaders only before and after the copy operation. Otherwise the more restrictive ORDER_PRIMITIVES would be necessary.</li>
<li>ORDER_FRAGMENTS_TILED can be used instead of ORDER_FRAGMENTS if and only if any copied texel is accessed only when processing fragments with the same (x,y) coordinates.</li>
<li>If application code is already using barriers for other purposes, overhead can be reduced by ordering rendering operations so that the same barriers can be used to order copy operations at the same time.</li>
</ul>
<p>Note that a single barrier can be specified with multiple bits. Please refer to the "Barriers" section of the NVN programming guide document for detailed description of the various barrier bits as well as usage guidelines and examples.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_13_3_2"></a>
2.13.3.2. Texture Copy with RSTR2D Unit</h4>
<p>Texture content can be copied through command generated by calling nvnCommandBufferCopyTextureToTexture(). Please refer to the NVN programming guide for details on how to use this function.</p>
<p>Copying texture with this method makes use of Maxwell's RSTR2D unit. Please see <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_unit_descriptions_RSTR2D">4.1.6. RSTR2D (2d Rasterizer)</a> for a description of this unit.</p>
<p>Texture copies which used the RSTR2D unit will always idle the 3D pipeline before the start of the texture copy and will not restart the 3D pipeline until the copy has been completed. This ensures that all previous draw calls that write to the source texture have completed and subsequent draw calls that sample from the destination texture will read the correct values.</p>
<p>Because idling the 3D pipeline can reduce performance we recommend the following to minimize its impact:</p><ul>
<li>If possible, execute the texture copy operations when the 3D pipeline is already idle.</li>
<li>Batch texture copy operations to share the cost of idling the 3D pipeline over as many texture copies as possible.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_13_3_3"></a>
2.13.3.3. Additional Performance Considerations</h4>
<p>Additional performance related considerations when using the texture copy functions are listed below:</p>
<ul>
<li>Texture copies with RSTR2D unit has maximum performance of 8 pixels-per-clock whereas texture copies with the 3D pipeline has maximum performance of 14.4 pixels-per-clock as limited by PROP throughput in the GPC (please see <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance">8.1. PERFORMANCE SUMMARY</a>). To achieve best performance it is recommended to use the 3D pipeline for texture copies.</li>
<li>The performance of texture copies of depth-stencil surfaces are lower than texture copies of color and depth-only surfaces. This is due to the layout of depth-stencil surfaces in memory where depth and stencil values are packed into separate blocks (please see <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_graphics_Z_AND_STENCIL_PACKING">5.5.1. Z and Stencil Packing</a>). This packing reduces the read/write efficiency of copy operations when compared to color and depth-only surfaces.</li>
</ul>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_14"></a>
2.14. Debug Group Annotation</h2>
<p>The methods nvnCommandBufferPushDebugGroup(), nvnCommandBufferPopDebugGroup(), and their variants incur the following costs which can affect performance:</p>
<ul>
<li>Inserting debug groups within command buffers increases command memory usage.</li>
<li>Processing of debug group data incurs additional cycles in the Front-End unit. This cost is heaviest when processing the nvnCommandBufferPushDebugGroupDynamic() variant where a single Front-End unit cycle is needed to process every 4 characters.</li>
<li>Debug group annotation function calls are considered 3D commands and must be drained from the pipeline before executing compute shaders. Please see <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_ComputeGraphicsInteractions">3.1. Interaction with Graphics Workload</a> for further details.</li>
</ul>
<p>Please note that the costs above are incurred for both develop and release libraries of the driver.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_14_1"></a>
2.14.1. Optimization Options</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_14_1_1"></a>
2.14.1.1. Remove Debug Group Annotations</h4>
<p>Although debug group annotations can be useful for investigating crashes and other issues, please be aware of the potential impact on performance as described above. If necessary to achieve performance goal, consider removing debug annotations for shipping/release versions of the application.</p>
<h1><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3"></a>
3. Compute</h1>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_ComputeGraphicsInteractions"></a>
3.1. Interaction with Graphics Workload</h2>
<p>The Maxwell architecture requires that the 3D pipeline be drained and idled before executing a compute shader. Similarly all compute shader executions must be finished before the 3D pipeline can be restarted.</p>
<p>Since this draining and idling can reduce performance we recommend the following when interleaving compute dispatch and graphics commands:</p><ul>
<li>If possible, execute compute dispatch calls when the 3D pipeline is already idle.</li>
<li>Batch compute dispatch operations to share the cost of idling the 3D pipeline over as many dispatches as possible.</li>
</ul>
<p>Please note that graphics commands, as mentioned above, are not limited to draw calls only. All NVN commands which change 3D states (e.g. uniform/SSBO bindings, color blend states, etc), are considered graphics commands. This also includes the nvnCommandBufferPushDebugGroup(), nvnCommandBufferPopDebugGroup(), and their variants.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3_2"></a>
3.2. Shaders</h2>
<p>Please also refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_GfxShader">2.7. Shader</a> as some suggestions there are also applicable to compute shaders.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3_2_1"></a>
3.2.1. UBO</h3>
<p>On NX, hardware constant accesses by compute shaders are limited to only the first 5 uniform blocks. Accesses to uniform blocks with an assigned binding number of 5 or higher will be emulated using global memory accesses, usually with lower access performance. We recommend that compute shaders use no more than 5 uniform blocks to avoid this performance penalty. If this is not possible, applications should assign its most frequently accessed uniform blocks to bindings 0 though 4 using "layout(binding=N)" to minimize the performance penalty. Another optimization option is to convert the compute dispatch call into a graphics draw call where the fragment shader can access up to 14 uniform blocks.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3_2_2"></a>
3.2.2. SSBO</h3>
<p>If threads within a dispatch call loads and stores from shader storage blocks, try to make the access pattern as coherent as possible. That is, the range of addresses being loaded or stored by all threads should be as small as possible to maximize performance.</p>
<p>If multiple threads need to write to sequential locations within a single SSBO, glsl atomic operations such as atomicAdd() can be used to reserve space and maximize memory access locality.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3_2_3"></a>
3.2.3. Shared Memory</h3>
<p>If accesses to shared memory becomes a performance limiter in compute shaders consider using the shuffleNV() instruction which allows exchange of values between threads in the same warp without incurring any access to shared memory. Please see the "shared" section of <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_WarpLimiters">2.7.4.2. Warp Throughput Limiters</a> for more on shared memory performance limiter based on static shader analysis. Please refer to NV_shader_thread_shuffle glsl extension for further details on shuffleNV() function. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- HTML footer for doxygen 1.9.1-->
<!-- start footer part -->
</body>
</html>
